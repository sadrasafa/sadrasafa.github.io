---
---
@INPROCEEDINGS{safadoust2024bmvc,
  author={Safadoust, Sadra and Tosi, Fabio and G端ney, Fatma and Poggi, Matteo},
  booktitle={Winter Conference on Applications of Computer Vision (WACV)},
  title={WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields},
  year={2026},
  abstract={We introduce WarpRF, a training-free general-purpose framework for quantifying the uncertainty of radiance fields. Built upon the assumption that photometric and geometric consistency should hold among images rendered by an accurate model, WarpRF quantifies its underlying uncertainty from an unseen point of view by leveraging backward warping across viewpoints, projecting reliable renderings to the unseen viewpoint and measuring the consistency with images rendered there. WarpRF is simple and inexpensive, does not require any training, and can be applied to any radiance field implementation for free. WarpRF excels at both uncertainty quantification and downstream tasks, e.g., active view selection and active mapping, outperforming any existing method tailored to specific frameworks.},
  preview={wacv26.gif},
  pdf={https://arxiv.org/pdf/2506.22433},
  project={https://kuis-ai.github.io/WarpRF},
  selected={true}}


@INPROCEEDINGS{safadoust2024bmvc,
  author={Safadoust, Sadra and Tosi, Fabio and G端ney, Fatma and Poggi, Matteo},
  booktitle={British Machine Vision Conference (BMVC)},
  title={Self-Evolving Depth-Supervised 3D Gaussian Splatting from Rendered Stereo Pairs},
  year={2024},
  abstract={3D Gaussian Splatting (GS) significantly struggles to accurately represent the underlying 3D scene geometry, resulting in inaccuracies and floating artifacts when rendering depth maps. In this paper, we address this limitation, undertaking a comprehensive analysis of the integration of depth priors throughout the optimization process of Gaussian primitives, and present a novel strategy for this purpose. This latter dynamically exploits depth cues from a readily available stereo network, processing virtual stereo pairs rendered by the GS model itself during training and achieving consistent self-improvement of the scene representation. Experimental results on three popular datasets, breaking ground as the first to assess depth accuracy for these models, validate our findings.},
  preview={bmvc24.gif},
  pdf={https://arxiv.org/pdf/2409.07456.pdf},
  project={https://kuis-ai.github.io/StereoGS},
  code={https://github.com/sadrasafa/StereoGS/},
  selected={true}}


@INPROCEEDINGS{safadoust2023iccv,
  author={Safadoust, Sadra and G端ney, Fatma},
  booktitle={International Conference on Computer Vision (ICCV)},
  title={Multi-Object Discovery by Low-Dimensional Object Motion},
  year={2023},
  abstract={Recent work in unsupervised multi-object segmentation shows impressive results by predicting motion from a single image despite the inherent ambiguity in predicting motion without the next image. On the other hand, the set of possible motions for an image can be constrained to a low-dimensional space by considering the scene structure and moving objects in it. We propose to model pixel-wise geometry and object motion to remove ambiguity in reconstructing flow from a single image. Specifically, we divide the image into coherently moving regions and use depth to construct flow bases that best explain the observed flow in each region. We achieve state-of-the-art results in unsupervised multi-object segmentation on synthetic and real-world datasets by modeling the scene structure and object motion. Our evaluation of the predicted depth maps shows reliable performance in monocular depth estimation.},
  preview={mos.gif},
  pdf={https://arxiv.org/pdf/2307.08027.pdf},
  project={https://kuis-ai.github.io/multi-object-segmentation},
  code={https://github.com/sadrasafa/multi-object-segmentation},
  selected={true}}

@article{safadoust2022depthpp,
  title={DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax},
  author={Safadoust, Sadra and G{\"u}ney, Fatma},
  journal={arXiv preprint},
  year={2023},
  abstract={Current self-supervised monocular depth estimation methods are mostly based on estimating a rigid-body motion representing camera motion. These methods suffer from the well-known scale ambiguity problem in their predictions. We propose DepthP+P, a method that learns to estimate outputs in metric scale by following the traditional planar parallax paradigm. We first align the two frames using a common ground plane which removes the effect of the rotation component in the camera motion. With two neural networks, we predict the depth and the camera translation, which is easier to predict alone compared to predicting it together with rotation. By assuming a known camera height, we can then calculate the induced 2D image motion of a 3D point and use it for reconstructing the target image in a self-supervised monocular approach. We perform experiments on the KITTI driving dataset and show that the planar parallax approach, which only needs to predict camera translation, can be a metrically accurate alternative to the current methods that rely on estimating 6DoF camera motion.},
  preview={depthpp.png},
  pdf={https://arxiv.org/pdf/2301.02092.pdf},
  selected={true}
}


@article{akan2022slamp3d,
  title={Stochastic Video Prediction with Structure and Motion},
  author={Akan, Adil Kaan and Safadoust, Sadra and Erdem, Erkut and Erdem, Aykut and G{\"u}ney, Fatma},
  journal={arXiv preprint},
  year={2022},
  abstract={While stochastic video prediction models enable future prediction under uncertainty, they mostly fail to model the complex dynamics of real-world scenes. For example, they cannot provide reliable predictions for scenes with a moving camera and independently moving foreground objects in driving scenarios. The existing methods fail to fully capture the dynamics of the structured world by only focusing on changes in pixels. In this paper, we assume that there is an underlying process creating observations in a video and propose to factorize it into static and dynamic components. We model the static part based on the scene structure and the ego-motion of the vehicle, and the dynamic part based on the remaining motion of the dynamic objects. By learning separate distributions of changes in foreground and background, we can decompose the scene into static and dynamic parts and separately model the change in each. Our experiments demonstrate that disentangling structure and motion helps stochastic video prediction, leading to better future predictions in complex driving scenarios on two real-world driving datasets, KITTI and Cityscapes.},
  preview={slamp3d.gif},
  pdf={https://arxiv.org/pdf/2203.10528.pdf},
  selected={true}
}


@INPROCEEDINGS{safadoust2021monodepthseg,
  author={Safadoust, Sadra and G端ney, Fatma},
  booktitle={International Conference on 3D Vision (3DV)},
  title={Self-Supervised Monocular Scene Decomposition and Depth Estimation},
  year={2021},
  abstract={Self-supervised monocular depth estimation approaches either ignore independently moving objects in the scene or need a separate segmentation step to identify them. We propose MonoDepthSeg to jointly estimate depth and segment moving objects from monocular video without using any ground-truth labels. We decompose the scene into a fixed number of components where each component corresponds to a region on the image with its own transformation matrix representing its motion. We estimate both the mask and the motion of each component efficiently with a shared encoder. We evaluate our method on three driving datasets and show that our model clearly improves depth estimation while decomposing the scene into separately moving components.},
  preview={monodepthseg.gif},
  pdf={https://arxiv.org/pdf/2110.11275.pdf},
  project={https://kuis-ai.github.io/monodepthseg},
  selected={true}}
  
