---
---

@INPROCEEDINGS{safadoust2021monodepthseg,
  author={Safadoust, Sadra and Güney, Fatma},
  booktitle={International Conference on Computer Vision (ICCV)},
  title={Multi-Object Discovery by Low-Dimensional Object Motion},
  year={2023},
  abstract={Recent work in unsupervised multi-object segmentation shows impressive results by predicting motion from a single image despite the inherent ambiguity in predicting motion without the next image. On the other hand, the set of possible motions for an image can be constrained to a low-dimensional space by considering the scene structure and moving objects in it. We propose to model pixel-wise geometry and object motion to remove ambiguity in reconstructing flow from a single image. Specifically, we divide the image into coherently moving regions and use depth to construct flow bases that best explain the observed flow in each region. We achieve state-of-the-art results in unsupervised multi-object segmentation on synthetic and real-world datasets by modeling the scene structure and object motion. Our evaluation of the predicted depth maps shows reliable performance in monocular depth estimation.},
  preview={mos.gif},
  pdf={https://arxiv.org/pdf/2307.08027.pdf},
  project={https://kuis-ai.github.io/multi-object-segmentation},
  arxiv={2307.08027},
  selected={true}}

@article{safadoust2022depthpp,
  title={DepthP+P: Metric Accurate Monocular Depth Estimation using Planar and Parallax},
  author={Safadoust, Sadra and G{\"u}ney, Fatma},
  journal={arXiv preprint},
  year={2023},
  abstract={Current self-supervised monocular depth estimation methods are mostly based on estimating a rigid-body motion representing camera motion. These methods suffer from the well-known scale ambiguity problem in their predictions. We propose DepthP+P, a method that learns to estimate outputs in metric scale by following the traditional planar parallax paradigm. We first align the two frames using a common ground plane which removes the effect of the rotation component in the camera motion. With two neural networks, we predict the depth and the camera translation, which is easier to predict alone compared to predicting it together with rotation. By assuming a known camera height, we can then calculate the induced 2D image motion of a 3D point and use it for reconstructing the target image in a self-supervised monocular approach. We perform experiments on the KITTI driving dataset and show that the planar parallax approach, which only needs to predict camera translation, can be a metrically accurate alternative to the current methods that rely on estimating 6DoF camera motion.},
  preview={depthpp.png},
  pdf={https://arxiv.org/pdf/2301.02092.pdf},
  arxiv={2301.02092},
  selected={true}
}


@article{akan2022slamp3d,
  title={Stochastic Video Prediction with Structure and Motion},
  author={Akan, Adil Kaan and Safadoust, Sadra and Erdem, Erkut and Erdem, Aykut and G{\"u}ney, Fatma},
  journal={arXiv preprint},
  year={2022},
  abstract={While stochastic video prediction models enable future prediction under uncertainty, they mostly fail to model the complex dynamics of real-world scenes. For example, they cannot provide reliable predictions for scenes with a moving camera and independently moving foreground objects in driving scenarios. The existing methods fail to fully capture the dynamics of the structured world by only focusing on changes in pixels. In this paper, we assume that there is an underlying process creating observations in a video and propose to factorize it into static and dynamic components. We model the static part based on the scene structure and the ego-motion of the vehicle, and the dynamic part based on the remaining motion of the dynamic objects. By learning separate distributions of changes in foreground and background, we can decompose the scene into static and dynamic parts and separately model the change in each. Our experiments demonstrate that disentangling structure and motion helps stochastic video prediction, leading to better future predictions in complex driving scenarios on two real-world driving datasets, KITTI and Cityscapes.},
  preview={slamp3d.gif},
  pdf={https://arxiv.org/pdf/2203.10528.pdf},
  arxiv={2203.10528},
  selected={true}
}


@INPROCEEDINGS{safadoust2021monodepthseg,
  author={Safadoust, Sadra and Güney, Fatma},
  booktitle={International Conference on 3D Vision (3DV)},
  title={Self-Supervised Monocular Scene Decomposition and Depth Estimation},
  year={2021},
  abstract={Self-supervised monocular depth estimation approaches either ignore independently moving objects in the scene or need a separate segmentation step to identify them. We propose MonoDepthSeg to jointly estimate depth and segment moving objects from monocular video without using any ground-truth labels. We decompose the scene into a fixed number of components where each component corresponds to a region on the image with its own transformation matrix representing its motion. We estimate both the mask and the motion of each component efficiently with a shared encoder. We evaluate our method on three driving datasets and show that our model clearly improves depth estimation while decomposing the scene into separately moving components.},
  preview={monodepthseg.gif},
  pdf={https://arxiv.org/pdf/2110.11275.pdf},
  project={https://kuis-ai.github.io/monodepthseg},
  arxiv={2110.11275},
  selected={true}}
  
